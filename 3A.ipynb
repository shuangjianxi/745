{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ddTCFLHwiiI",
        "outputId": "302ab9b0-bd62-45ac-d1db-961083c0e33c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        " #setting path in colab\n",
        " #mount the drive first\n",
        " from google.colab import drive\n",
        " drive.mount('/content/drive', force_remount= True)\n",
        "\n",
        "#click on the URL and copy the code. paste it in the box and press enter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorflow-io"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxNxecvCzd8K",
        "outputId": "e120caf5-4684-49ab-93d0-74dda18c70d2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-io\n",
            "  Downloading tensorflow_io-0.37.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem==0.37.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-io) (0.37.0)\n",
            "Installing collected packages: tensorflow-io\n",
            "Successfully installed tensorflow-io-0.37.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "#set the working directory\n",
        "\n",
        "root_dir = \"/content/drive/My Drive/\"\n",
        "\n",
        "#choose the project folder\n",
        "project_folder = \"Colab Notebooks/Project_folder/\"\n",
        "\n",
        "#define a function to create and set the working directory\n",
        "def create_and_set_working_directory(project_folder):\n",
        "  #check if the project folder exists. if not, make one.\n",
        "  if os.path.isdir(root_dir + project_folder) == False:\n",
        "    os.mkdir(root_dir + project_folder)\n",
        "    print(root_dir + project_folder + 'did not existed and was created.')\n",
        "\n",
        "  #change the OS path to project folder as working directory\n",
        "  os.chdir(root_dir + project_folder)\n",
        "\n",
        "  #create a test file in the working directory and see if it shows up at the right place\n",
        "  !touch 'new_file_test.txt'\n",
        "  print('working directory' + root_dir + project_folder + \\\n",
        "        \"empty text file created. You can also run !pwd command to confirm working directory.\")\n",
        "\n",
        "create_and_set_working_directory(project_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tMhs1ABwzzB",
        "outputId": "4ce12ddf-28d4-4ec0-96e1-19605bf280fe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "working directory/content/drive/My Drive/Colab Notebooks/Project_folder/empty text file created. You can also run !pwd command to confirm working directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_io as tfio\n",
        "from tensorflow.io import parse_single_example\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "#Mark the nominal columns and consolidate the data (extract all the nominal columns)\n",
        "#标出名词性列，整合数据（把名词性的列都提取出来）\n",
        "def combine_dataset(files, col_names, processed = False):\n",
        "\tdtypes = {}\n",
        "\tif processed == False:\n",
        "\t\tfor col_name in col_names:\n",
        "\t\t\tnominal_names = set(['srcip', 'sport', 'dstip', 'dsport', 'proto', 'state',\n",
        "\t\t\t\t                 'service', 'ct_ftp', 'label_10'])  #Nominal column\n",
        "\t\t\tif col_name in nominal_names:\n",
        "\t\t\t\tdtypes[col_name] =  str\n",
        "\t\t\telse:\n",
        "\t\t\t\tdtypes[col_name] = np.float32\n",
        "\telse:\n",
        "\t\tfor col_name in col_names:\n",
        "\t\t\tdtypes[col_name] = np.float32\n",
        "\n",
        "\trecords = []\n",
        "\tfor file in files:\n",
        "\t\tdata = pd.read_csv(file, header = None, names = col_names, dtype = dtypes)\n",
        "\t\trecords.append(data)\n",
        "\n",
        "\trecords_all = pd.concat(records)#当没有索引时、concat不管列名，直接加到一起\n",
        "                                     #When there is no index, concat adds them together regardless of the column names,\n",
        "\n",
        "\n",
        "\treturn records_all\n",
        "\n",
        "## Make new col names for categorical features after one-hot encoding\n",
        "#为one-hot编码之后的列起个新列名\n",
        "def get_nominal_names(dataset, cols_nominal):\n",
        "\tdata_nominal = dataset[cols_nominal]\n",
        "\n",
        "\n",
        "\n",
        "\tnew_col_names = []\n",
        "\tfor col_name in cols_nominal:\n",
        "\t\tname_unique = sorted(dataset[col_name].unique())  #名词性列的不同的值。Different values for noun columns\n",
        "\t\tnew_col_name = [col_name + '_' + x for x in name_unique]\n",
        "\t\tnew_col_names.extend(new_col_name)\n",
        "\n",
        "\treturn new_col_names\n",
        "\n",
        "#Remove the unimportant feature, one-hot encoding, and convert the attack class to numeric\n",
        "#删除不重要的特征，one-hot编码，将攻击类别转换为数值型\n",
        "def select_feature_and_encoding(dataset, cols_to_drop, cols_nominal, cols_nominal_all):\n",
        "\n",
        "\n",
        "\t# Drop the features has no meaning such as src ip. 删除不重要的特征\n",
        "\tfor cols in cols_to_drop:\n",
        "\t\tdataset.drop(cols, axis = 1, inplace = True)\n",
        "\n",
        "\t# Save the label and then drop it from dataset 保留标签然后将它从数据集中删除（提取出标签列）\n",
        "\tlabel_10 = dataset['label_10']\n",
        "\tlabel_2 = dataset['label_2']\n",
        "\tdataset.drop('label_2', axis = 1, inplace = True)\n",
        "\tdataset.drop('label_10', axis = 1, inplace = True)\n",
        "\n",
        "\t# replace the label with specific code  将标签数值化\n",
        "\treplace_dict = { np.nan: 0, 'Analysis': 1, 'Backdoors': 2, 'Backdoor': 2, 'DoS': 3,\n",
        "                    'Exploits':4,' Fuzzers': 5, ' Fuzzers ':5, 'Generic': 6,\n",
        "                    'Reconnaissance': 7, ' Shellcode ':8, 'Shellcode': 8,\n",
        "                    'Worms':9, ' Reconnaissance ': 7,}\n",
        "\tnew_label_10 = label_10.replace(replace_dict)\n",
        "\tnew_label_10.to_frame()\n",
        "\tlabel_2.to_frame\n",
        "\tdel label_10\n",
        "\n",
        "\t# replace the lost values  用0替换缺失值\n",
        "\treplace_dict = {np.nan: 0, ' ': 0}\n",
        "\tfor cols in ['ct_ftp', 'ct_flw', 'is_ftp']:\n",
        "\t\tdataset[cols] = dataset[cols].replace(replace_dict)\n",
        "\n",
        "\t# 'is_ftp' column is wrong, correct it(I found that the value of it is\n",
        "\t# all the same with ct_ftp_cmd, so if the value is not 0, is_ftp should\n",
        "\t# be 1)\n",
        "\tfor x in dataset['is_ftp']:\n",
        "\t\tif x != 0:\n",
        "\t\t\tx = 1\n",
        "\n",
        "\t# select and process the categorical features 选择并处理分类特征\n",
        "\tdata_nominal = dataset[cols_nominal]  #cols_nominal是名词性列的列名，提取出名词性列的数据\n",
        "\tdata_temp_1 = data_nominal.apply(LabelEncoder().fit_transform)  #将名词性列进行编号\n",
        "\tdel data_nominal\n",
        "\n",
        "\n",
        "\tnew_col_names = []\n",
        "\tfor col_name in cols_nominal:\n",
        "\t\tname_unique = sorted(dataset[col_name].unique())\n",
        "\t\tnew_col_name = [col_name + '_' + x for x in name_unique]\n",
        "\n",
        "\t\tnew_col_names.extend(new_col_name)\n",
        "\t\tdataset.drop(col_name, axis = 1, inplace = True)\n",
        "\n",
        "\t#one-hot\n",
        "\tenc = OneHotEncoder()\n",
        "\tdata_temp_2 = enc.fit_transform(data_temp_1)\n",
        "\tdel data_temp_1\n",
        "\n",
        "\tdata_encoded = pd.DataFrame(data_temp_2.toarray(), columns = new_col_names)\n",
        "\tdel data_temp_2\n",
        "\n",
        "\t# complement the nominal columns 补充名词性列\n",
        "\tdiff = set(cols_nominal_all) - set(new_col_names)\n",
        "\n",
        "\tif diff:\n",
        "\t\tfor cols in diff:\n",
        "\t\t\tdata_encoded[cols] = 0.\n",
        "\t\tdata_encoded = data_encoded[cols_nominal_all]\n",
        "\n",
        "\tdataset= dataset.join(data_encoded)\n",
        "\tdel data_encoded\n",
        "\n",
        "\tdataset = dataset.join(new_label_10)\n",
        "\tdataset = dataset.join(label_2)\n",
        "\n",
        "\treturn dataset  #Complete data set (including data and labels)\n",
        "                    #完整的数据集（包括数据和标签）\n",
        "\n",
        "#Split the training set and test set and save the file as a CSV file\n",
        "#分裂训练集和测试集,并将文件保存成CSV文件\n",
        "def split_dataset(dataset, file_train, file_test):\n",
        "\n",
        "\tcols = dataset.columns\n",
        "\t#trainset, testset = train_test_split(dataset, test_size = 0.2)\n",
        "\ttrainset, testset = train_test_split(dataset, test_size = 0.2,random_state=40,stratify=dataset['label_10'])\n",
        "\ttrain = pd.DataFrame(trainset, columns = cols)\n",
        "\ttest = pd.DataFrame(testset, columns = cols)\n",
        "\n",
        "\ttrain.to_csv(file_train)\n",
        "\ttest.to_csv(file_test)\n",
        "\n",
        "#Standardize, and save the file in CSV and tf formats\n",
        "#标准化，并将文件保存成csv格式和tf格式\n",
        "def scaling(files_train, files_test, col_names_scaling, scaling_type):\n",
        "\n",
        "\tif scaling_type == 'min_max':\n",
        "\t\tscaler = MinMaxScaler()\n",
        "\t\tfile_folder = 'min_max/'\n",
        "\telse:\n",
        "\t\tscaler = StandardScaler()\n",
        "\t\tfile_folder = 'normalized/'\n",
        "\n",
        "\tif not os.path.exists(file_folder):\n",
        "\t\tos.mkdir(file_folder)\n",
        "\tcols = []\n",
        "\tfor file in files_train:\n",
        "\t\t# col 0 is the index in the file\n",
        "\t\ttrainset = pd.read_csv(file, index_col = 0, dtype = np.float32)\n",
        "\t\tif len(cols) == 0:\n",
        "\t\t\tcols = trainset.columns\n",
        "\t\tscaler.partial_fit(trainset[col_names_scaling])\n",
        "\n",
        "\tdel trainset\n",
        "\tcols_keep = list(set(cols) - set(col_names_scaling))\n",
        "\n",
        "\tfor file in files_train:\n",
        "\t\ttrainset = pd.read_csv(file, dtype = np.float32)\n",
        "\t\ttrain_scaled = scaler.transform(trainset[col_names_scaling])\n",
        "\t\ttrain_changed = pd.DataFrame(train_scaled, columns = col_names_scaling)\n",
        "\t\ttrain_unchanged = trainset[cols_keep]\n",
        "\t\ttrainset_final = pd.concat((train_changed, train_unchanged),\n",
        "\t\t                        axis = 1)\n",
        "\t\ttrainset_final = trainset_final[cols]\n",
        "\t\tprint(\"train:\",trainset_final.shape)  #trainset shape\n",
        "\t\tfile_csv = file_folder + file\n",
        "\t\ttrainset.to_csv(file_csv, index = False)\n",
        "\t\tlen_tail = len('.csv')\n",
        "\t\tfile_tfr = file_folder + file[:-1 * len_tail] + '.tfrecords'\n",
        "\t\tmake_tfrecords(trainset_final, file_tfr)\n",
        "\n",
        "\tfor file in files_test:\n",
        "\t\ttestset = pd.read_csv(file, dtype = np.float32)\n",
        "\t\ttest_scaled = scaler.transform(testset[col_names_scaling])\n",
        "\t\ttest_changed = pd.DataFrame(test_scaled, columns = col_names_scaling)\n",
        "\t\ttest_unchanged = testset[cols_keep]\n",
        "\t\ttestset_final = pd.concat((test_changed, test_unchanged),axis = 1)\n",
        "\t\ttestset_final = testset_final[cols]\n",
        "\t\tprint(\"test:\",testset_final.shape)\n",
        "\t\tfile_csv = file_folder + file\n",
        "\t\ttestset.to_csv(file_csv, index = False)\n",
        "\t\tlen_tail = len('.csv')\n",
        "\t\tfile_tfr = file_folder + file[:-1 * len_tail] + '.tfrecords'\n",
        "\t\tmake_tfrecords(testset_final, file_tfr)\n",
        "\n",
        "#Save the file in tf format\n",
        "#将文件保存成tf格式\n",
        "# def make_tfrecords(dataset, file_to_save):\n",
        "\n",
        "# \ttry:\n",
        "# \t\tdata = dataset.values\n",
        "# \texcept:\n",
        "# \t\tdata = dataset\n",
        "# \twith tf.python_io.TFRecordWriter(file_to_save) as writer:\n",
        "# \t\tfor rows in data:\n",
        "# \t\t\tfeatures, label_10, label_2 = rows[:-2], rows[-2], rows[-1]\n",
        "# \t\t\tfeature = {'features': tf.train.Feature(float_list = tf.train.FloatList(value = features)),\n",
        "# \t\t\t           'label_2': tf.train.Feature(float_list = tf.train.FloatList(value = [label_2])),\n",
        "# \t\t\t           'label_10': tf.train.Feature(float_list = tf.train.FloatList(value = [label_10]))}\n",
        "# \t\t\texample = tf.train.Example(features = tf.train.Features(feature = feature))\n",
        "# \t\t\twriter.write(example.SerializeToString())\n",
        "\n",
        "def make_tfrecords(dataset, file_to_save):\n",
        "\n",
        "    try:\n",
        "        data = dataset.values\n",
        "    except:\n",
        "        data = dataset\n",
        "    with tf.io.TFRecordWriter(file_to_save) as writer:\n",
        "        for rows in data:\n",
        "            features, label_10, label_2 = rows[:-2], rows[-2], rows[-1]\n",
        "            feature = {'features': tf.train.Feature(float_list = tf.train.FloatList(value = features)),\n",
        "                       'label_2': tf.train.Feature(float_list = tf.train.FloatList(value = [label_2])),\n",
        "                       'label_10': tf.train.Feature(float_list = tf.train.FloatList(value = [label_10]))}\n",
        "            example = tf.train.Example(features = tf.train.Features(feature = feature))\n",
        "            writer.write(example.SerializeToString())\n",
        "\n",
        "# def next_batch(filename, batch_size):\n",
        "\n",
        "# \tlen_feature = 202  #特征数（不包含标签）。 Number of features (not including tags)\n",
        "# \tlen_label = 1#标签长度。 The length of the label\n",
        "\n",
        "# \tdef read_data(examples):\n",
        "# \t\tfeatures = {\"features\": tf.io.FixedLenFeature([len_feature], tf.float32),\n",
        "#                     \"label_2\": tf.io.FixedLenFeature([len_label], tf.float32),\n",
        "#                     \"label_10\": tf.io.FixedLenFeature([len_label], tf.float32)}\n",
        "# \t\tparsed_features = tf.io.parse_single_example(examples, features)\n",
        "# \t\treturn parsed_features['features'], parsed_features['label_2'], \\\n",
        "#                parsed_features['label_10']\n",
        "\n",
        "# \tdata = tf.data.TFRecordDataset(filename)\n",
        "# \tdata = data.map(read_data)\n",
        "# \tdata = data.batch(batch_size)\n",
        "# \titerator = data.make_initializable_iterator()\n",
        "# \tnext_data, next_label_2, next_label_10 = iterator.get_next()\n",
        "\n",
        "# \treturn next_data, next_label_10, next_label_2\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def next_batch(filename, batch_size):\n",
        "\n",
        "    len_feature = 202  # Number of features (not including labels)\n",
        "    len_label = 1  # The length of the label\n",
        "\n",
        "    def read_data(examples):\n",
        "        features = {\n",
        "            \"features\": tf.io.FixedLenFeature([len_feature], tf.float32),\n",
        "            \"label_2\": tf.io.FixedLenFeature([len_label], tf.float32),\n",
        "            \"label_10\": tf.io.FixedLenFeature([len_label], tf.float32)\n",
        "        }\n",
        "        parsed_features = tf.io.parse_single_example(examples, features)\n",
        "        return parsed_features['features'], parsed_features['label_2'], parsed_features['label_10']\n",
        "\n",
        "    # Create a TFRecordDataset\n",
        "    data = tf.data.TFRecordDataset(filename)\n",
        "    data = data.map(read_data)\n",
        "    data = data.batch(batch_size)\n",
        "\n",
        "    # Create an iterator\n",
        "    iterator = iter(data)\n",
        "\n",
        "    # Fetch the next batch\n",
        "    try:\n",
        "        next_data, next_label_2, next_label_10 = next(iterator)\n",
        "    except StopIteration:\n",
        "        next_data, next_label_2, next_label_10 = None, None, None\n",
        "\n",
        "    return next_data, next_label_10, next_label_2\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\tfile_folder = root_dir + project_folder  #读取的原始文件所在的位置。 The location where the original file was read\n",
        "\tcol_names = ['srcip', 'sport', 'dstip', 'dsport', 'proto', 'state', 'dur',\n",
        "\t             'sbytes', 'dbytes', 'sttl', 'dttl', 'sloss', 'dloss',\n",
        "\t             'service', 'sload', 'dload', 'spkts', 'dpkts', 'swin', 'dwin',\n",
        "\t             'stcpb', 'dtcpb', 'smeansz', 'dmeansz', 'trans_depth',\n",
        "\t             'res_bdy_len', 'sjit', 'djit', 'stime', 'ltime', 'sintpkt',\n",
        "\t             'dintpkt', 'tcprtt', 'synack', 'ackdat', 'is_sm_ips',\n",
        "\t             'ct_state_ttl', 'ct_flw', 'is_ftp', 'ct_ftp', 'ct_srv_src',\n",
        "\t             'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ltm', 'ct_src_dport',\n",
        "\t             'ct_dst_sport', 'ct_dst_src', 'label_10', 'label_2']    #特证名（列名）。 listed name\n",
        "\n",
        "\tcols_to_drop = ['srcip', 'dstip', 'stime', 'ltime', 'sport', 'dsport']\n",
        "\tcols_nominal = ['proto', 'service', 'state']   #名词性特征。Nominal features\n",
        "\n",
        "\tfiles = [file_folder + 'UNSW-NB15_' + str(i+1) + '.csv' for i in range(4)]\n",
        "\tdataset = combine_dataset(files, col_names)\n",
        "\tcols_nominal_all = get_nominal_names(dataset, cols_nominal)\n",
        "\tdel dataset\n",
        "\n",
        "\tfile_tail = len('.csv')\n",
        "\tfile_head = len(file_folder + 'UNSW-NB15_')\n",
        "\tdtypes = {}\n",
        "\tfor col_name in col_names:\n",
        "\t\tnominal_names = set(['srcip', 'sport', 'dstip', 'dsport', 'proto', 'state',\n",
        "\t\t\t                 'service', 'is_ftp', 'ct_flw', 'ct_ftp', 'label_10'])\n",
        "\t\tif col_name in nominal_names:\n",
        "\t\t\tdtypes[col_name] =  str\n",
        "\t\telse:\n",
        "\t\t\tdtypes[col_name] = np.float32\n",
        "\n",
        "\tfor file in files:\n",
        "\t\tfile_train = file[file_head:-1 * file_tail] + '_train.csv'  #每个文件分裂出的训练集和测试集，csv文件。\n",
        "        #Each file is split out of the training set and test set, CSV file\n",
        "\t\tfile_test = file[file_head: -1 * file_tail] + '_test.csv'\n",
        "\t\tdataset = pd.read_csv(file, header = None, names = col_names, dtype = dtypes)\n",
        "\t\tdataset = select_feature_and_encoding(dataset, cols_to_drop, cols_nominal,\n",
        "\t\t                                          cols_nominal_all)\n",
        "\t\tsplit_dataset(dataset, file_train, file_test)\n",
        "\n",
        "\tcols_unchanged = ['is_ftp', 'is_sm_ips'] + cols_nominal +\\\n",
        "\t                 cols_to_drop + ['label_2', 'label_10']\n",
        "\tcols_scaling = [x for x in col_names if x not in cols_unchanged]\n",
        "\n",
        "\tfiles_train = [str(x + 1) + '_train.csv' for x in range(4)]\n",
        "\tfiles_test = [str(x + 1) + '_test.csv' for x in range(4)]\n",
        "\n",
        "\tscaling(files_train, files_test, cols_scaling, 'std')  #标准化。standardized\n",
        "\n",
        "\tfile_folder = 'normalized/' #数据标准化后存放的文件夹。A folder where data is stored after standardization\n",
        "\tfiles_train = [file_folder + str(x + 1) + '_train.tfrecords' for x in range(4)]\n",
        "\tfiles_test = [file_folder + str(x + 1) + '_test.tfrecords' for x in range(4)]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Cj1ZPDUy13U",
        "outputId": "e56c2e11-607a-4da9-86db-dbeba55a55e5"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: (560000, 204)\n",
            "train: (560000, 204)\n",
            "train: (560000, 204)\n",
            "train: (352035, 204)\n",
            "test: (140001, 204)\n",
            "test: (140001, 204)\n",
            "test: (140001, 204)\n",
            "test: (88009, 204)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #Integrate the four separate data sets\n",
        "# #将分开的4个数据集整合到一起\n",
        "# def make_whole_datasets(tfrecords_train, num_train_example, tfrecords_test,\n",
        "#                         num_test_example):\n",
        "\n",
        "#     data_train, label_10_train, label_2_train= next_batch(tfrecords_train,num_train_example)\n",
        "#     data_test, label_10_test, label_2_test= next_batch(tfrecords_test,num_test_example)\n",
        "#     with tf.Session() as sess:\n",
        "#         data, label_10, label_2 = sess.run([data_train, label_10_train,label_2_train])\n",
        "#     dataset = np.concatenate([data, label_10, label_2], axis = 1)\n",
        "\n",
        "#     #trainset, valiset = train_test_split(dataset, test_size = 254004,stratify=dataset['label_10'])\n",
        "#     trainset, valiset = train_test_split(dataset, test_size = 0.125,random_state=40,stratify=dataset[:,-2])\n",
        "#     print(\"train:\",trainset.shape)\n",
        "#     print(\"val:\",valiset.shape)\n",
        "\n",
        "#     make_tfrecords(trainset, 'normalized/train.tfrecords')\n",
        "#     make_tfrecords(valiset, 'normalized/validation.tfrecords')\n",
        "\n",
        "#     del trainset, valiset\n",
        "\n",
        "#     with tf.Session() as sess:\n",
        "#         data, label_10, label_2 = sess.run([data_test, label_10_test,label_2_test])\n",
        "#     dataset = np.concatenate([data, label_10, label_2], axis = 1)\n",
        "#     print(\"test:\",dataset.shape)\n",
        "#     make_tfrecords(dataset, 'normalized/test.tfrecords')\n",
        "\n",
        "def make_whole_datasets(tfrecords_train, num_train_example, tfrecords_test, num_test_example):\n",
        "    # Get the data and labels for training and testing\n",
        "    data_train, label_10_train, label_2_train = next_batch(tfrecords_train, num_train_example)\n",
        "    data_test, label_10_test, label_2_test = next_batch(tfrecords_test, num_test_example)\n",
        "\n",
        "    # Combine data and labels\n",
        "    dataset_train = np.concatenate([data_train.numpy(), label_10_train.numpy(), label_2_train.numpy()], axis=1)\n",
        "    dataset_test = np.concatenate([data_test.numpy(), label_10_test.numpy(), label_2_test.numpy()], axis=1)\n",
        "\n",
        "    # Split the training set into training and validation sets\n",
        "    trainset, valiset = train_test_split(dataset_train, test_size=0.125, random_state=40, stratify=dataset_train[:,-2])\n",
        "    print(\"train:\", trainset.shape)\n",
        "    print(\"val:\", valiset.shape)\n",
        "\n",
        "    # Save the training and validation sets to TFRecord files\n",
        "    make_tfrecords(trainset, 'normalized/train.tfrecords')\n",
        "    make_tfrecords(valiset, 'normalized/validation.tfrecords')\n",
        "\n",
        "    del trainset, valiset\n",
        "\n",
        "    # Save the test set to a TFRecord file\n",
        "    print(\"test:\", dataset_test.shape)\n",
        "    make_tfrecords(dataset_test, 'normalized/test.tfrecords')"
      ],
      "metadata": {
        "id": "2m9Y1CZb4w06"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_train_example = 2032035 #trainset size\n",
        "num_test_example = 508012 #testset size\n",
        "make_whole_datasets(files_train, num_train_example, files_test, num_test_example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ik-iOPNu4ygS",
        "outputId": "6d0c535a-9e20-490a-f10c-0fcd2eebf192"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: (1778030, 204)\n",
            "val: (254005, 204)\n",
            "test: (508012, 204)\n"
          ]
        }
      ]
    }
  ]
}